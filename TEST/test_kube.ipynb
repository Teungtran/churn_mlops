{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import zipfile\n",
    "import shutil\n",
    "import joblib as jb\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, NamedTuple\n",
    "from collections import Counter,namedtuple\n",
    "import os\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Artifact,\n",
    "    Model,\n",
    "    Metrics,\n",
    "    component\n",
    ")\n",
    "from box.exceptions import BoxValueError\n",
    "import yaml\n",
    "import json\n",
    "import joblib\n",
    "from ensure import ensure_annotations\n",
    "from box import ConfigBox\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score, matthews_corrcoef\n",
    ")\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "try:\n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage import transfer_manager\n",
    "    CLOUD_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CLOUD_AVAILABLE = False\n",
    "    print(\"Google Cloud Storage not available - install with: pip install google-cloud-storage\")\n",
    "\n",
    "# ================================================================\n",
    "# LOGGING SETUP\n",
    "# ================================================================\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='[%(asctime)s]: %(message)s:')\n",
    "logger = logging.getLogger(__name__)\n",
    "logs_dir = \"logs\"\n",
    "log_filepath = os.path.join(logs_dir, \"running_logs.log\")\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_filepath,\n",
    "    format=\"[%(asctime)s]: %(levelname)s: %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filepath),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"MLopsLogger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ensure_annotations\n",
    "def read_yaml(path_to_yaml: Path) -> ConfigBox:\n",
    "    \"\"\"reads yaml file and returns\n",
    "\n",
    "    Args:\n",
    "        path_to_yaml (str): path like input\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if yaml file is empty\n",
    "        e: empty file\n",
    "\n",
    "    Returns:\n",
    "        ConfigBox: ConfigBox type\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(path_to_yaml) as yaml_file:\n",
    "            content = yaml.safe_load(yaml_file)\n",
    "            logger.info(f\"yaml file: {path_to_yaml} loaded successfully\")\n",
    "            return ConfigBox(content)\n",
    "    except BoxValueError:\n",
    "        raise ValueError(\"yaml file is empty\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    \n",
    "\n",
    "\n",
    "@ensure_annotations\n",
    "def create_directories(path_to_directories: list, verbose=True):\n",
    "    \"\"\"create list of directories\n",
    "\n",
    "    Args:\n",
    "        path_to_directories (list): list of path of directories\n",
    "        ignore_log (bool, optional): ignore if multiple dirs is to be created. Defaults to False.\n",
    "    \"\"\"\n",
    "    for path in path_to_directories:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        if verbose:\n",
    "            logger.info(f\"created directory at: {path}\")\n",
    "\n",
    "\n",
    "@ensure_annotations\n",
    "def save_json(path: Path, data: dict):\n",
    "    \"\"\"save json data\n",
    "\n",
    "    Args:\n",
    "        path (Path): path to json file\n",
    "        data (dict): data to be saved in json file\n",
    "    \"\"\"\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    logger.info(f\"json file saved at: {path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@ensure_annotations\n",
    "def load_json(path: Path) -> ConfigBox:\n",
    "    \"\"\"load json files data\n",
    "\n",
    "    Args:\n",
    "        path (Path): path to json file\n",
    "\n",
    "    Returns:\n",
    "        ConfigBox: data as class attributes instead of dict\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        content = json.load(f)\n",
    "\n",
    "    logger.info(f\"json file loaded succesfully from: {path}\")\n",
    "    return ConfigBox(content)\n",
    "\n",
    "\n",
    "@ensure_annotations\n",
    "def save_bin(data: Any, path: Path):\n",
    "    \"\"\"save binary file\n",
    "\n",
    "    Args:\n",
    "        data (Any): data to be saved as binary\n",
    "        path (Path): path to binary file\n",
    "    \"\"\"\n",
    "    joblib.dump(value=data, filename=path)\n",
    "    logger.info(f\"binary file saved at: {path}\")\n",
    "\n",
    "\n",
    "@ensure_annotations\n",
    "def load_bin(path: Path) -> Any:\n",
    "    \"\"\"load binary data\n",
    "\n",
    "    Args:\n",
    "        path (Path): path to binary file\n",
    "\n",
    "    Returns:\n",
    "        Any: object stored in the file\n",
    "    \"\"\"\n",
    "    data = joblib.load(path)\n",
    "    logger.info(f\"binary file loaded from: {path}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "@ensure_annotations\n",
    "def get_size(path: Path) -> str:\n",
    "    \"\"\"get size in KB\n",
    "\n",
    "    Args:\n",
    "        path (Path): path of the file\n",
    "\n",
    "    Returns:\n",
    "        str: size in KB\n",
    "    \"\"\"\n",
    "    size_in_kb = round(os.path.getsize(path)/1024)\n",
    "    return f\"~ {size_in_kb} KB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_many_blobs_with_transfer_manager(\n",
    "    bucket_name, filenames, source_directory=\"\", workers=8, credentials_path=None\n",
    "):\n",
    "    \"\"\"Upload multiple files to Google Cloud Storage with proper error handling.\"\"\"\n",
    "    if not CLOUD_AVAILABLE:\n",
    "        logger.warning(\"Google Cloud Storage not available - skipping upload\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        # Set GCP credentials if provided\n",
    "        if credentials_path:\n",
    "            os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_path\n",
    "        \n",
    "        if isinstance(filenames, (Path, str)):\n",
    "            filenames = [str(filenames)]\n",
    "        else:\n",
    "            filenames = [str(f) for f in filenames]\n",
    "        \n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(source_directory, filename) if source_directory else filename\n",
    "            if not os.path.exists(file_path):\n",
    "                logger.warning(f\"File does not exist for upload: {file_path}\")\n",
    "                continue\n",
    "        \n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        results = transfer_manager.upload_many_from_filenames(\n",
    "            bucket, filenames, source_directory=source_directory, max_workers=workers, blob_name_prefix=\"churn_data_store/\"\n",
    "        )\n",
    "\n",
    "        for name, result in zip(filenames, results):\n",
    "            if isinstance(result, Exception):\n",
    "                logger.error(f\"Failed to upload {name}: {result}\")\n",
    "            else:\n",
    "                logger.info(f\"Uploaded {name} to bucket {bucket.name}.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Cloud storage upload failed: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    data_version_dir: Path\n",
    "    local_data_file: Path\n",
    "    test_size: float\n",
    "    random_state: int\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PrepareBaseModelConfig:\n",
    "    model_version_dir: Path\n",
    "    data_version_dir: Path\n",
    "    random_state: int\n",
    "    n_estimators: int\n",
    "    criterion: str\n",
    "    max_depth: int\n",
    "    max_features: str\n",
    "    min_samples_leaf: int\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    model_version_dir: Path\n",
    "    data_version_dir: Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    model_version_dir: Path\n",
    "    data_version_dir: Path\n",
    "    plots_dir: Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CloudStoragePushConfig:\n",
    "    root_dir: Path\n",
    "    bucket_name: str\n",
    "    credentials_path: str\n",
    "    data_version_dir: Path\n",
    "    evaluation_dir: Path\n",
    "    workers: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=Path(r\"C:\\Users\\Admin\\Desktop\\Data projects\\python\\Decision-making-system\\churn_mlops\\config\\config.yaml\")\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "        \n",
    "        create_directories([config.root_dir, config.data_version_dir])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            local_data_file=Path(config.local_data_file),\n",
    "            test_size=config.test_size,\n",
    "            random_state=config.random_state,\n",
    "            data_version_dir=Path(config.data_version_dir)\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Data Ingestion config: {config}\")\n",
    "        return data_ingestion_config\n",
    "        \n",
    "    def get_prepare_base_model_config(self) -> PrepareBaseModelConfig:\n",
    "        config = self.config.prepare_base_model\n",
    "        \n",
    "        create_directories([config.model_version_dir, config.data_version_dir])\n",
    "\n",
    "        prepare_base_model_config = PrepareBaseModelConfig(\n",
    "            model_version_dir=Path(config.model_version_dir),\n",
    "            data_version_dir=Path(config.data_version_dir),\n",
    "            n_estimators=config.n_estimators,\n",
    "            random_state=config.random_state,\n",
    "            criterion=config.criterion,\n",
    "            max_depth=config.max_depth,\n",
    "            max_features=config.max_features,\n",
    "            min_samples_leaf=config.min_samples_leaf\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Prepare base model config: {config}\")\n",
    "        return prepare_base_model_config\n",
    "\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        config = self.config.training\n",
    "        \n",
    "        create_directories([config.model_version_dir, config.data_version_dir])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            model_version_dir=Path(config.model_version_dir),\n",
    "            data_version_dir=Path(config.data_version_dir),\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Training config: {config}\")\n",
    "        return training_config\n",
    "\n",
    "    def get_evaluation_config(self) -> EvaluationConfig:\n",
    "        config = self.config.evaluation\n",
    "        \n",
    "        create_directories([config.plots_dir, config.model_version_dir, config.data_version_dir])\n",
    "\n",
    "        evaluation_config = EvaluationConfig(\n",
    "            model_version_dir=Path(config.model_version_dir),\n",
    "            data_version_dir=Path(config.data_version_dir),\n",
    "            plots_dir=Path(config.plots_dir)\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Evaluation config: {config}\")\n",
    "        return evaluation_config\n",
    "\n",
    "    def get_cloud_storage_push_config(self) -> CloudStoragePushConfig:\n",
    "        config = self.config.cloud_storage_push\n",
    "        \n",
    "        cloud_storage_push_config = CloudStoragePushConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            bucket_name=config.bucket_name,\n",
    "            credentials_path=config.credentials_path,\n",
    "            data_version_dir=Path(config.data_version_dir),\n",
    "            evaluation_dir=Path(config.evaluation_dir),\n",
    "            workers=config.workers\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Cloud Storage Push config: {config}\")\n",
    "        return cloud_storage_push_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies(df):\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        for col in categorical_cols:\n",
    "            if df[col].isin(['yes', 'no', 'True', 'False']).any():\n",
    "                df[col] = df[col].map({'yes': 1, 'True': 1, 'no': 0, 'False': 0})\n",
    "            else:\n",
    "                df = pd.get_dummies(df, columns=[col])\n",
    "    return df\n",
    "\n",
    "def most_common(lst):\n",
    "    counts = Counter(lst)\n",
    "    if not counts:\n",
    "        return None \n",
    "    return counts.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "        self.rows_processed = 0\n",
    "        self.datetime_suffix = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "\n",
    "    def process_data_for_churn(self, df_input):\n",
    "        df_input.columns = df_input.columns.map(lambda x: str(x).strip())\n",
    "        cols_to_drop = {\"Returns\", \"Age\", \"Total Purchase Amount\"}\n",
    "        df_input.drop(columns=[col for col in cols_to_drop if col in df_input.columns], inplace=True)\n",
    "        df_input.dropna(inplace=True)\n",
    "        if 'Price' not in df_input.columns:\n",
    "            df_input['Price'] = df_input['Product Price']\n",
    "        if 'Product Price' not in df_input.columns:\n",
    "            raise KeyError(\"Required column 'Product Price' is missing from the dataset.\")\n",
    "        \n",
    "        df_input['TotalSpent'] = df_input['Quantity'] * df_input['Price']\n",
    "        df_features = df_input.groupby(\"customer_id\", as_index=False, sort=False).agg(\n",
    "            LastPurchaseDate = (\"Purchase Date\",\"max\"),\n",
    "            Favoured_Product_Categories = (\"Product Category\", lambda x: most_common(list(x))),\n",
    "            Frequency = (\"Purchase Date\", \"count\"),\n",
    "            TotalSpent = (\"TotalSpent\", \"sum\"),\n",
    "            Favoured_Payment_Methods = (\"Payment Method\", lambda x: most_common(list(x))),\n",
    "            Customer_Name = (\"Customer Name\", \"first\"),\n",
    "            Customer_Label = (\"Customer_Labels\", \"first\"),\n",
    "            Churn = (\"Churn\", \"first\"),\n",
    "        )\n",
    "\n",
    "        df_features = df_features.drop_duplicates(subset=['Customer_Name'], keep='first')\n",
    "        df_features['LastPurchaseDate'] = pd.to_datetime(df_features['LastPurchaseDate'])\n",
    "        df_features['LastPurchaseDate'] = df_features['LastPurchaseDate'].dt.date\n",
    "        df_features['LastPurchaseDate'] = pd.to_datetime(df_features['LastPurchaseDate'])\n",
    "        max_LastBuyingDate = df_features[\"LastPurchaseDate\"].max()\n",
    "        df_features['Recency'] = (max_LastBuyingDate - df_features['LastPurchaseDate']).dt.days\n",
    "        df_features['LastPurchaseDate'] = df_features['LastPurchaseDate'].dt.date\n",
    "        df_features['Avg_Spend_Per_Purchase'] = df_features['TotalSpent'] / df_features['Frequency'].replace(0, 1)\n",
    "        df_features['Purchase_Consistency'] = df_features['Recency'] / df_features['Frequency'].replace(0, 1)\n",
    "        df_features.drop(columns=[\"customer_id\",\"LastPurchaseDate\",'Customer_Name'], inplace=True)\n",
    "        \n",
    "        return df_features\n",
    "\n",
    "    def encode_churn(self, df_features: pd.DataFrame):\n",
    "        df_copy = df_features.copy()\n",
    "        df_features_encode = get_dummies(df_copy)\n",
    "        return df_features_encode\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load the dataset from CSV file and save versioned copy.\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading data from {self.config.local_data_file}\")\n",
    "            df = pd.read_csv(self.config.local_data_file)\n",
    "            \n",
    "            input_data_versioned_name = f\"input_raw_data_version_{self.datetime_suffix}.csv\"\n",
    "            input_data_versioned_path = Path(self.config.data_version_dir) / input_data_versioned_name\n",
    "            if not input_data_versioned_path.exists():\n",
    "                df.to_csv(input_data_versioned_path, index=False)\n",
    "                logger.info(f\"Created versioned input data file: {input_data_versioned_path}\")\n",
    "            else:\n",
    "                logger.info(f\"Versioned file already exists: {input_data_versioned_path}, skipping save.\")\n",
    "\n",
    "            logger.info(f\"Loaded dataset with {len(df)} rows\")\n",
    "            logger.info(f\"Columns found: {list(df.columns)}\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error while loading data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def preprocess_data(self, df_clean):\n",
    "        self.rows_processed = 0\n",
    "        logger.info(f\"Starting preprocessing of {len(df_clean)} rows...\")\n",
    "        \n",
    "        df_clean = self.process_data_for_churn(df_clean)\n",
    "        logger.info(f\"After feature engineering columns: {list(df_clean.columns)}\")\n",
    "        \n",
    "        df_clean = self.encode_churn(df_clean)\n",
    "        logger.info(f\"After encoding columns: {list(df_clean.columns)}\")\n",
    "        \n",
    "        df_clean = df_clean.dropna()\n",
    "        logger.info(f\"Data shape after removing NaN: {df_clean.shape}\")\n",
    "        \n",
    "        if \"Churn\" not in df_clean.columns:\n",
    "            logger.error(f\"Churn column not found! Available columns: {list(df_clean.columns)}\")\n",
    "            raise KeyError(\"Churn column is missing after preprocessing\")\n",
    "        \n",
    "        X = df_clean.drop(\"Churn\", axis=1)\n",
    "        y = df_clean[\"Churn\"]\n",
    "        \n",
    "        logger.info(f\"X shape (features): {X.shape}\")\n",
    "        logger.info(f\"y shape (target): {y.shape}\")\n",
    "        \n",
    "        if y.isna().sum() > 0:\n",
    "            logger.warning(f\"Found {y.isna().sum()} NaN values in target variable, filling with 0\")\n",
    "            y = y.fillna(0)\n",
    "        \n",
    "        nan_cols = X.columns[X.isna().any()].tolist()\n",
    "        if nan_cols:\n",
    "            logger.warning(f\"Found NaN values in feature columns: {nan_cols}\")\n",
    "            X = X.fillna(0)\n",
    "        \n",
    "        class_distribution = y.value_counts(normalize=True)\n",
    "        logger.info(f\"Target variable distribution (normalized): \\n{class_distribution}\")\n",
    "\n",
    "        imbalance_threshold = 0.4\n",
    "        if class_distribution.min() < imbalance_threshold:\n",
    "            logger.info(\"Target variable is imbalanced. Applying SMOTEENN...\")\n",
    "            smote = SMOTEENN(random_state=42)\n",
    "            X_res, y_res = smote.fit_resample(X, y)\n",
    "            logger.info(f\"Resampled feature matrix shape: {X_res.shape}\")\n",
    "        else:\n",
    "            logger.info(\"Target variable is balanced. Skipping SMOTEENN.\")\n",
    "            X_res, y_res = X, y\n",
    "        return X_res, y_res\n",
    "\n",
    "    def split_data(self, X_res, y_res):\n",
    "        \"\"\"Split data into training and testing sets.\"\"\"\n",
    "        logger.info(\"Splitting data into train and test sets\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_res, y_res, test_size=self.config.test_size, random_state=self.config.random_state\n",
    "        )\n",
    "        logger.info(f\"Train data: {X_train.shape}, Test data: {X_test.shape}\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def save_data(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"Save training and testing data to CSV files.\"\"\"\n",
    "        logger.info(\"Saving processed feature data to versioned directory\")\n",
    "        \n",
    "        os.makedirs(self.config.data_version_dir, exist_ok=True)\n",
    "        \n",
    "        train_feature_path = os.path.join(self.config.data_version_dir, f\"train_feature_version_{self.datetime_suffix}.csv\")\n",
    "        test_feature_path = os.path.join(self.config.data_version_dir, f\"test_feature_version_{self.datetime_suffix}.csv\")\n",
    "        train_target_path = os.path.join(self.config.data_version_dir, f\"train_target_version_{self.datetime_suffix}.csv\")\n",
    "        test_target_path = os.path.join(self.config.data_version_dir, f\"test_target_version_{self.datetime_suffix}.csv\")\n",
    "        \n",
    "        X_train.to_csv(train_feature_path, index=False)\n",
    "        X_test.to_csv(test_feature_path, index=False)\n",
    "        y_train.to_csv(train_target_path, index=False, header=['Churn'])\n",
    "        y_test.to_csv(test_target_path, index=False, header=['Churn'])\n",
    "        \n",
    "        try:\n",
    "            versioned_files = [\n",
    "                f\"train_feature_version_{self.datetime_suffix}.csv\",\n",
    "                f\"test_feature_version_{self.datetime_suffix}.csv\", \n",
    "                f\"train_target_version_{self.datetime_suffix}.csv\",\n",
    "                f\"test_target_version_{self.datetime_suffix}.csv\"\n",
    "            ]\n",
    "            \n",
    "            upload_many_blobs_with_transfer_manager(\n",
    "                bucket_name=\"churn_data_version\",  # You can modify this\n",
    "                filenames=versioned_files,\n",
    "                source_directory=str(self.config.data_version_dir),\n",
    "                workers=8\n",
    "            )\n",
    "            logger.info(\"Successfully uploaded versioned training data to Google Cloud Storage\")\n",
    "\n",
    "        except Exception as cloud_error:\n",
    "            logger.warning(f\"Failed to upload training data to cloud storage: {cloud_error}\")\n",
    "        \n",
    "        logger.info(f\"Processed features saved to:\")\n",
    "        logger.info(f\"  X_train: {train_feature_path}\")\n",
    "        logger.info(f\"  X_test: {test_feature_path}\")\n",
    "        logger.info(f\"  y_train: {train_target_path}\")\n",
    "        logger.info(f\"  y_test: {test_target_path}\")\n",
    "        \n",
    "        return train_feature_path, test_feature_path, train_target_path, test_target_path\n",
    "\n",
    "    def data_ingestion_pipeline(self):\n",
    "        \"\"\"Main method to perform data ingestion.\"\"\"\n",
    "        logger.info(\"Initiating data ingestion\")\n",
    "        df = self.load_data()\n",
    "        X, y = self.preprocess_data(df)\n",
    "        X_train, X_test, y_train, y_test = self.split_data(X, y)\n",
    "        train_target, test_target, y_train_path, y_test_path = self.save_data(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        logger.info(\"Data ingestion completed successfully\")\n",
    "        return X_train, X_test, y_train, y_test, train_target, test_target, y_train_path, y_test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareBaseModel:\n",
    "    def __init__(self, config: PrepareBaseModelConfig):\n",
    "        self.config = config\n",
    "        # Generate datetime suffix for this run\n",
    "        self.datetime_suffix = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "        \n",
    "    def get_base_model(self):\n",
    "        \"\"\"Create and return a base Random Forest model for churn prediction.\"\"\"\n",
    "        logger.info(\"Creating base Random Forest model\")\n",
    "        \n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=self.config.n_estimators, \n",
    "            random_state=self.config.random_state,\n",
    "            criterion=self.config.criterion,\n",
    "            max_depth=self.config.max_depth,\n",
    "            max_features=self.config.max_features,\n",
    "            min_samples_leaf=self.config.min_samples_leaf\n",
    "        )\n",
    "        logger.info(f\"Model params:{self.config.n_estimators}, {self.config.random_state}, {self.config.criterion}, {self.config.max_depth}, {self.config.max_features}, {self.config.min_samples_leaf}\")\n",
    "        return model\n",
    "    \n",
    "    def scaler(self, train_path, test_path, y_train_path, y_test_path):\n",
    "        \"\"\"Prepare and save scaler based on training data.\"\"\"\n",
    "        logger.info(\"Creating scaler from training data\")\n",
    "        \n",
    "        os.makedirs(self.config.data_version_dir, exist_ok=True)\n",
    "        os.makedirs(self.config.model_version_dir, exist_ok=True)\n",
    "        \n",
    "        scaled_test_data_path = os.path.join(self.config.data_version_dir, f\"test_feature_scaled_version_{self.datetime_suffix}.csv\")\n",
    "        scaled_train_data_path = os.path.join(self.config.data_version_dir, f\"train_feature_scaled_version_{self.datetime_suffix}.csv\")\n",
    "        scaler_path = os.path.join(self.config.model_version_dir, f\"scaler_churn_version_{self.datetime_suffix}.pkl\")\n",
    "        \n",
    "        try:\n",
    "            X_train = pd.read_csv(train_path)\n",
    "            X_test = pd.read_csv(test_path)\n",
    "            y_train = pd.read_csv(y_train_path)\n",
    "            y_test = pd.read_csv(y_test_path)\n",
    "            \n",
    "            scaler = RobustScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            jb.dump(scaler, scaler_path)\n",
    "            \n",
    "            X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "            X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    \n",
    "            X_train_scaled_df.to_csv(scaled_train_data_path, index=False)\n",
    "            X_test_scaled_df.to_csv(scaled_test_data_path, index=False)\n",
    "            \n",
    "            logger.info(f\"Scalers and scaled data saved:\")\n",
    "            logger.info(f\"  Scaler: {scaler_path}\")\n",
    "            logger.info(f\"  X_train_scaled: {scaled_train_data_path}\")\n",
    "            logger.info(f\"  X_test_scaled: {scaled_test_data_path}\")\n",
    "            \n",
    "            return scaler, X_train, X_test, y_train, y_test, scaled_train_data_path, scaled_test_data_path, scaler_path\n",
    " \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in preparing scaler: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def full_model(self, train_path, test_path, y_train_path, y_test_path):\n",
    "        \"\"\"Create the base model and scaler.\"\"\"\n",
    "        logger.info(\"Creating base model and scaler\")\n",
    "        \n",
    "        model = self.get_base_model()\n",
    "        scaler, X_train, X_test, y_train, y_test, scaled_train_path, scaled_test_path, scaler_path = self.scaler(\n",
    "            train_path, test_path, y_train_path, y_test_path\n",
    "        )\n",
    "        \n",
    "        base_model_path = os.path.join(self.config.model_version_dir, f\"base_model_churn_{self.datetime_suffix}.pkl\")\n",
    "        \n",
    "        # Save base model\n",
    "        jb.dump(model, base_model_path)\n",
    "        \n",
    "        logger.info(f\"Base model saved: {base_model_path}\")\n",
    "\n",
    "        return model, scaler, X_train, X_test, y_train, y_test, base_model_path, scaled_train_path, scaled_test_path, scaler_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndEvaluateModel:\n",
    "    def __init__(self, config_train: TrainingConfig, config_eval: EvaluationConfig = None):\n",
    "        self.train_config = config_train\n",
    "        self.eval_config = config_eval\n",
    "        self.datetime_suffix = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "        self.model_name = f\"model_churn_{self.datetime_suffix}\"\n",
    "        self.fine_tuned_model_name = f\"finetuned_churn_{self.datetime_suffix}\"\n",
    "    \n",
    "    def train(self, X_train_scaled, y_train, base_model_path):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        logger.info(f\"Loading model from: {base_model_path}\")\n",
    "        model = jb.load(base_model_path)\n",
    "        logger.info(f\"Starting model training for {self.model_name}\")\n",
    "        model = model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        logger.info(f\"Model training for {self.model_name} completed\")\n",
    "        model_version_dir = str(self.train_config.model_version_dir)\n",
    "        os.makedirs(model_version_dir, exist_ok=True)\n",
    "        \n",
    "        trained_model_path_versioned = os.path.join(model_version_dir, f\"model_churn_version_{self.datetime_suffix}.pkl\")\n",
    "        \n",
    "        jb.dump(model, trained_model_path_versioned)\n",
    "        logger.info(f\"  trained model file (for future use): {trained_model_path_versioned}\")\n",
    "        return model, trained_model_path_versioned\n",
    "    \n",
    "    def fine_tune(self, model, X_train_scaled, y_train):\n",
    "        \"\"\"Fine-tune the exact trained model with hyperparameter search.\"\"\"\n",
    "        logger.info(\"Starting fine-tuning of the trained model\")\n",
    "        \n",
    "        rf_params = {\n",
    "            'n_estimators': [100, 200, 300, 400, 500, 700, 1000],  \n",
    "            'criterion': ['gini', 'entropy', 'log_loss'],          # log_loss for classification since sklearn 1.1+\n",
    "            'max_depth': [None, 10, 20, 30, 50, 70],                # Include deeper trees\n",
    "            'min_samples_split': [2, 5, 10, 15],                    # More control over overfitting\n",
    "            'min_samples_leaf': [1, 2, 4, 6],                       # Helps with generalization\n",
    "            'max_features': ['sqrt', 'log2', None],                # 'auto' is deprecated; None = all features\n",
    "            'bootstrap': [True, False],                             # Evaluate both bootstrapped and full datasets\n",
    "            'class_weight': [None, 'balanced', 'balanced_subsample']  # Handles imbalanced datasets\n",
    "        }\n",
    "\n",
    "        logger.info(f\"Fine-tuning model with current parameters: n_estimators={model.n_estimators}, criterion={model.criterion}\")\n",
    "        random_search = RandomizedSearchCV(model, rf_params, cv=5, n_jobs=1, n_iter=20, random_state=42)\n",
    "        random_search.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        best_model = RandomForestClassifier(**random_search.best_params_, random_state=42)\n",
    "        logger.info(f\"Best parameters found: {random_search.best_params_}\")\n",
    "        logger.info(f\"Best cross-validation score: {random_search.best_score_:.4f}\")\n",
    "        \n",
    "        # Fit the best model with the training data\n",
    "        best_model = best_model.fit(X_train_scaled, y_train)\n",
    "        logger.info(\"Best model fitted with training data\")\n",
    "\n",
    "        model_version_dir = str(self.train_config.model_version_dir)\n",
    "        os.makedirs(model_version_dir, exist_ok=True)\n",
    "        \n",
    "        fine_tuned_model_path_versioned = os.path.join(model_version_dir, f\"finetuned_churn_{self.datetime_suffix}.pkl\")\n",
    "        \n",
    "        jb.dump(best_model, fine_tuned_model_path_versioned)\n",
    "        \n",
    "        logger.info(f\"Fine-tuned models saved:\")\n",
    "        logger.info(f\"  Versioned file (for future use): {fine_tuned_model_path_versioned}\")\n",
    "        \n",
    "        return best_model, fine_tuned_model_path_versioned\n",
    "    \n",
    "    def perform_detailed_evaluation(self, model, X_test_scaled, y_test):\n",
    "        \"\"\"Evaluate the model in detail and save metrics.\"\"\"\n",
    "        logger.info(\"Performing detailed evaluation on test data\")\n",
    "\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        avg_precision = average_precision_score(y_test, y_pred_prob)\n",
    "        \n",
    "        metrics = {\n",
    "            \"precision\": float(precision),\n",
    "            \"recall\": float(recall),\n",
    "            \"f1_score\": float(f1),\n",
    "            \"roc_auc\": float(roc_auc),\n",
    "            \"mcc\": float(mcc),\n",
    "            \"avg_precision\": float(avg_precision)\n",
    "        }\n",
    "        \n",
    "        # Generate classification report\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        metrics[\"classification_report\"] = report\n",
    "        \n",
    "        metrics_file_versioned = Path(self.eval_config.plots_dir) / f\"metrics_{self.datetime_suffix}.json\"\n",
    "        \n",
    "        save_json(metrics_file_versioned, metrics)\n",
    "        \n",
    "        logger.info(f\"Metrics saved:\")\n",
    "        logger.info(f\"  Versioned file (for future use): {metrics_file_versioned}\")\n",
    "        \n",
    "        return metrics, y_pred, y_pred_prob\n",
    "    \n",
    "    def plot_confusion_matrix(self, y_test, y_pred):\n",
    "        \"\"\"Plot and save confusion matrix.\"\"\"\n",
    "        logger.info(\"Creating confusion matrix plot\")\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        \n",
    "        # Save the plot with datetime naming\n",
    "        cm_path = os.path.join(self.eval_config.plots_dir, f\"confusion_matrix_{self.datetime_suffix}.png\")\n",
    "        plt.savefig(cm_path)\n",
    "        plt.close()\n",
    "        logger.info(f\"Confusion matrix saved to: {cm_path}\")\n",
    "        \n",
    "        return cm_path\n",
    "    def plot_precision_recall_curve(self, y_test, y_pred_prob):\n",
    "        \"\"\"Plot and save Precision-Recall curve.\"\"\"\n",
    "        logger.info(\"Creating Precision-Recall curve plot\")\n",
    "\n",
    "        precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "        avg_precision = average_precision_score(y_test, y_pred_prob)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(recall_vals, precision_vals, color='purple', lw=2,\n",
    "                label=f'Precision-Recall curve (AP = {avg_precision:.3f})')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend(loc='lower left')\n",
    "\n",
    "        # Save plot with datetime suffix naming\n",
    "        pr_path = os.path.join(self.eval_config.plots_dir, f\"precision_recall_curve_{self.datetime_suffix}.png\")\n",
    "        plt.savefig(pr_path)\n",
    "        plt.close()\n",
    "        logger.info(f\"Precision-Recall curve saved to: {pr_path}\")\n",
    "\n",
    "        return pr_path\n",
    "    def plot_roc_curve(self, y_test, y_pred_prob):\n",
    "        \"\"\"Plot and save ROC curve.\"\"\"\n",
    "        logger.info(\"Creating ROC curve plot\")\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        \n",
    "        # Save the plot with datetime naming\n",
    "        roc_path = os.path.join(self.eval_config.plots_dir, f\"roc_curve_{self.datetime_suffix}.png\")\n",
    "        plt.savefig(roc_path)\n",
    "        plt.close()\n",
    "        logger.info(f\"ROC curve saved to: {roc_path}\")\n",
    "        \n",
    "        return roc_path\n",
    "    \n",
    "    def train_and_evaluate(self, base_model_path, scaled_train_path, scaled_test_path, y_train_path, y_test_path):\n",
    "        \"\"\"Main method to train and evaluate the model.\"\"\"\n",
    "        logger.info(\"Initiating model training and evaluation\")\n",
    "        \n",
    "        X_train_scaled = pd.read_csv(scaled_train_path)\n",
    "        X_test_scaled = pd.read_csv(scaled_test_path)\n",
    "\n",
    "        try:\n",
    "            y_train = pd.read_csv(y_train_path)['Churn']\n",
    "            y_test = pd.read_csv(y_test_path)['Churn']\n",
    "            \n",
    "            logger.info(f\"Loaded y_train shape: {y_train.shape}\")\n",
    "            logger.info(f\"Loaded y_test shape: {y_test.shape}\")\n",
    "            \n",
    "            # Handle NaN values in target variables\n",
    "            logger.info(f\"NaN count in y_train: {y_train.isna().sum()}\")\n",
    "            logger.info(f\"NaN count in y_test: {y_test.isna().sum()}\")\n",
    "            \n",
    "            if y_train.isna().sum() > 0:\n",
    "                logger.warning(f\"Found {y_train.isna().sum()} NaN values in y_train, filling with 0\")\n",
    "                y_train = y_train.fillna(0)\n",
    "                \n",
    "            if y_test.isna().sum() > 0:\n",
    "                logger.warning(f\"Found {y_test.isna().sum()} NaN values in y_test, filling with 0\")\n",
    "                y_test = y_test.fillna(0)\n",
    "                \n",
    "        except FileNotFoundError as e:\n",
    "            logger.error(f\"Target files not found: {e}\")\n",
    "            raise e\n",
    "\n",
    "        logger.info(f\"Final X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "        logger.info(f\"Final y_train shape: {y_train.shape}\")\n",
    "        logger.info(f\"Final X_test_scaled shape: {X_test_scaled.shape}\")\n",
    "        logger.info(f\"Final y_test shape: {y_test.shape}\")\n",
    "\n",
    "        if X_train_scaled.isna().sum().sum() > 0:\n",
    "            logger.warning(f\"Found NaN values in X_train_scaled, filling with 0\")\n",
    "            X_train_scaled = X_train_scaled.fillna(0)\n",
    "            \n",
    "        if X_test_scaled.isna().sum().sum() > 0:\n",
    "            logger.warning(f\"Found NaN values in X_test_scaled, filling with 0\")\n",
    "            X_test_scaled = X_test_scaled.fillna(0)\n",
    "\n",
    "        if y_train.isna().sum() > 0:\n",
    "            logger.error(f\"Still have NaN values in y_train: {y_train.isna().sum()}\")\n",
    "            y_train = y_train.fillna(0)\n",
    "            \n",
    "        if y_test.isna().sum() > 0:\n",
    "            logger.error(f\"Still have NaN values in y_test: {y_test.isna().sum()}\")\n",
    "            y_test = y_test.fillna(0)\n",
    "\n",
    "        model, trained_model_path_static = self.train(X_train_scaled, y_train, base_model_path)\n",
    "        accuracy = model.score(X_test_scaled, y_test)\n",
    "        logger.info(f\"Model accuracy on test data: {accuracy}\")\n",
    "        \n",
    "        if accuracy < 0.85:\n",
    "            logger.info(\"Model accuracy is less than 85%, fine-tuning needed\")\n",
    "            fine_tuned_model, fine_tuned_model_path_static = self.fine_tune(model, X_train_scaled, y_train)\n",
    "            os.makedirs(self.eval_config.plots_dir, exist_ok=True)\n",
    "\n",
    "            detailed_metrics, y_pred, y_pred_prob = self.perform_detailed_evaluation(fine_tuned_model, X_test_scaled, y_test)\n",
    "\n",
    "            cm_path = self.plot_confusion_matrix(y_test, y_pred)\n",
    "            per_path = self.plot_precision_recall_curve(y_test, y_pred_prob)\n",
    "            roc_path = self.plot_roc_curve(y_test, y_pred_prob)\n",
    "            accuracy = fine_tuned_model.score(X_test_scaled, y_test)\n",
    "            logger.info(f\"Model accuracy on test data after fine-tuned: {accuracy}\")\n",
    "\n",
    "            return fine_tuned_model, detailed_metrics, fine_tuned_model_path_static\n",
    "        \n",
    "        else:\n",
    "            logger.info(\"Model accuracy is 85% or above, not needed finetuned\")\n",
    "            os.makedirs(self.eval_config.plots_dir, exist_ok=True)\n",
    "\n",
    "            detailed_metrics, y_pred, y_pred_prob = self.perform_detailed_evaluation(model, X_test_scaled, y_test)\n",
    "            \n",
    "            cm_path = self.plot_confusion_matrix(y_test, y_pred)\n",
    "            per_path = self.plot_precision_recall_curve(y_test, y_pred_prob)\n",
    "            roc_path = self.plot_roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "            return model, detailed_metrics, trained_model_path_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudStoragePush:\n",
    "    def __init__(self, config: CloudStoragePushConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def validate_bucket_exists(self):\n",
    "        \"\"\"Validate that the GCP bucket exists before attempting upload.\"\"\"\n",
    "        try:\n",
    "            # Set GCP credentials from configuration\n",
    "            os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = self.config.credentials_path\n",
    "            \n",
    "            from google.cloud import storage\n",
    "            client = storage.Client()\n",
    "            bucket = client.bucket(self.config.bucket_name)\n",
    "            bucket.reload()\n",
    "            logger.info(f\"Bucket '{self.config.bucket_name}' exists and is accessible\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Bucket validation failed: {e}\")\n",
    "            logger.error(f\"Bucket '{self.config.bucket_name}' does not exist or is not accessible\")\n",
    "            return False\n",
    "\n",
    "    def get_files_to_upload(self):\n",
    "        \"\"\"Get all files that need to be uploaded to cloud storage.\"\"\"\n",
    "        files_to_upload = []\n",
    "        \n",
    "        # Only upload from data_version_dir\n",
    "        if os.path.exists(self.config.data_version_dir):\n",
    "            data_files = glob.glob(os.path.join(self.config.data_version_dir, \"**\"), recursive=True)\n",
    "            for file_path in data_files:\n",
    "                if os.path.isfile(file_path):\n",
    "                    files_to_upload.append(file_path)\n",
    "                    \n",
    "        # Only upload from evaluation_dir\n",
    "        if os.path.exists(self.config.evaluation_dir):\n",
    "            eval_files = glob.glob(os.path.join(self.config.evaluation_dir, \"**\"), recursive=True)\n",
    "            for file_path in eval_files:\n",
    "                if os.path.isfile(file_path):\n",
    "                    files_to_upload.append(file_path)\n",
    "        \n",
    "        return files_to_upload\n",
    "\n",
    "    def push_to_cloud_storage(self):\n",
    "        \"\"\"Push all artifacts to GCP cloud storage.\"\"\"\n",
    "        try:\n",
    "            os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = self.config.credentials_path\n",
    "            \n",
    "            logger.info(\"Starting cloud storage push process...\")\n",
    "            \n",
    "            if not self.validate_bucket_exists():\n",
    "                error_msg = (\n",
    "                    f\"CLOUD STORAGE SETUP REQUIRED:\\n\"\n",
    "                    f\"1. Create GCP bucket '{self.config.bucket_name}' in your Google Cloud project\\n\"\n",
    "                    f\"2. Set up GCP credentials using one of these methods:\\n\"\n",
    "                    f\"   - Run: gcloud auth application-default login\\n\"\n",
    "                    f\"   - Set GOOGLE_APPLICATION_CREDENTIALS environment variable\\n\"\n",
    "                    f\"   - Use service account key file\\n\"\n",
    "                    f\"3. Ensure you have storage.objects.create permission on the bucket\"\n",
    "                )\n",
    "                raise Exception(error_msg)\n",
    "            \n",
    "            files_to_upload = self.get_files_to_upload()\n",
    "            \n",
    "            if not files_to_upload:\n",
    "                logger.warning(\"No files found to upload to cloud storage.\")\n",
    "                return\n",
    "                \n",
    "            logger.info(f\"Found {len(files_to_upload)} files to upload to cloud storage.\")\n",
    "            \n",
    "            relative_files = []\n",
    "            for file_path in files_to_upload:\n",
    "                rel_path = os.path.relpath(file_path, self.config.root_dir)\n",
    "                relative_files.append(rel_path)\n",
    "            \n",
    "            upload_many_blobs_with_transfer_manager(\n",
    "                bucket_name=self.config.bucket_name,\n",
    "                filenames=relative_files,\n",
    "                source_directory=self.config.root_dir,\n",
    "                workers=self.config.workers,\n",
    "                credentials_path=self.config.credentials_path\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Successfully uploaded {len(files_to_upload)} files to GCP bucket: {self.config.bucket_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to push to cloud storage: {e}\")\n",
    "            raise e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_temp_files():\n",
    "    \"\"\"Clean up temporary versioned files after pipeline completion.\"\"\"\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(\"CLEANUP: Removing temporary versioned files\")\n",
    "    logger.info(\"=\" * 50)\n",
    "    config_manager = ConfigurationManager()\n",
    "    config_manager.config.evaluation.plots_dir\n",
    "    data_version_dir = config_manager.config.data_ingestion.data_version_dir\n",
    "    evaluation_dir = config_manager.config.evaluation.plots_dir\n",
    "    try:\n",
    "        # Pattern: *_version_YYYYMMDDTHHMMSS.csv\n",
    "        data_version_files = glob.glob(os.path.join(data_version_dir, \"*_version_????????T??????.csv\"))\n",
    "        logger.info(f\"Found {len(data_version_files)} timestamp-versioned data files to clean\")\n",
    "        \n",
    "        for file_path in data_version_files:\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                logger.info(f\"Deleted temporary file: {os.path.basename(file_path)}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to delete file {file_path}: {e}\")\n",
    "        \n",
    "        # Pattern: *_YYYYMMDDTHHMMSS.json and *_YYYYMMDDTHHMMSS.png\n",
    "        eval_json_files = glob.glob(os.path.join(evaluation_dir, \"*_????????T??????.json\"))\n",
    "        eval_png_files = glob.glob(os.path.join(evaluation_dir, \"*_????????T??????.png\"))\n",
    "        eval_files = eval_json_files + eval_png_files\n",
    "        logger.info(f\"Found {len(eval_files)} timestamp-versioned evaluation files to clean\")\n",
    "        \n",
    "        for file_path in eval_files:\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                logger.info(f\"Deleted temporary file: {os.path.basename(file_path)}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to delete file {file_path}: {e}\")\n",
    "        \n",
    "        remaining_data_files = len(glob.glob(os.path.join(data_version_dir, \"*.csv\")))\n",
    "        remaining_eval_files = len(glob.glob(os.path.join(evaluation_dir, \"*\")))\n",
    "        logger.info(f\"Kept {remaining_data_files} essential data files for DVC tracking\")\n",
    "        logger.info(f\"Kept {remaining_eval_files} essential evaluation files for DVC tracking\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error during cleanup: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"numpy\", \"joblib\"]\n",
    ")\n",
    "def data_preparation_component(\n",
    "    train_data: Output[Artifact],\n",
    "    test_data: Output[Artifact], \n",
    "    y_train_data: Output[Artifact],\n",
    "    y_test_data: Output[Artifact]\n",
    ") -> NamedTuple('Outputs', [('train_path', str), ('test_path', str), ('y_train_path', str), ('y_test_path', str)]):\n",
    "    \"\"\"Data preparation component for churn prediction using existing DataPreparationStage\"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO, format='[%(asctime)s]: %(message)s:')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logs_dir = \"logs\"\n",
    "    log_filepath = os.path.join(logs_dir, \"running_logs.log\")\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        filename=log_filepath,\n",
    "        format=\"[%(asctime)s]: %(levelname)s: %(message)s\",\n",
    "        level=logging.INFO,\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filepath),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger(\"MLopsLogger\")\n",
    "    \n",
    "    logger.info(\">>> Stage Data Ingestion started <<<\")\n",
    "    STAGE_NAME = \"Data Ingestion stage\"\n",
    "\n",
    "    logger.info(f\">>> Stage {STAGE_NAME} started <<<\")\n",
    "    \n",
    "    class ConfigurationManager:\n",
    "        def __init__(\n",
    "            self,\n",
    "            config_filepath=Path(r\"C:\\Users\\Admin\\Desktop\\Data projects\\python\\Decision-making-system\\churn_mlops\\config\\config.yaml\")\n",
    "        ):\n",
    "            self.config = read_yaml(config_filepath)\n",
    "            create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "        def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "            config = self.config.data_ingestion\n",
    "            \n",
    "            create_directories([config.root_dir, config.data_version_dir])\n",
    "\n",
    "            data_ingestion_config = DataIngestionConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                local_data_file=Path(config.local_data_file),\n",
    "                test_size=config.test_size,\n",
    "                random_state=config.random_state,\n",
    "                data_version_dir=Path(config.data_version_dir)\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Data Ingestion config: {config}\")\n",
    "            return data_ingestion_config\n",
    "            \n",
    "        def get_prepare_base_model_config(self) -> PrepareBaseModelConfig:\n",
    "            config = self.config.prepare_base_model\n",
    "            \n",
    "            create_directories([config.model_version_dir, config.data_version_dir])\n",
    "\n",
    "            prepare_base_model_config = PrepareBaseModelConfig(\n",
    "                model_version_dir=Path(config.model_version_dir),\n",
    "                data_version_dir=Path(config.data_version_dir),\n",
    "                n_estimators=config.n_estimators,\n",
    "                random_state=config.random_state,\n",
    "                criterion=config.criterion,\n",
    "                max_depth=config.max_depth,\n",
    "                max_features=config.max_features,\n",
    "                min_samples_leaf=config.min_samples_leaf\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Prepare base model config: {config}\")\n",
    "            return prepare_base_model_config\n",
    "\n",
    "\n",
    "        def get_training_config(self) -> TrainingConfig:\n",
    "            config = self.config.training\n",
    "            \n",
    "            create_directories([config.model_version_dir, config.data_version_dir])\n",
    "\n",
    "            training_config = TrainingConfig(\n",
    "                model_version_dir=Path(config.model_version_dir),\n",
    "                data_version_dir=Path(config.data_version_dir),\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Training config: {config}\")\n",
    "            return training_config\n",
    "\n",
    "        def get_evaluation_config(self) -> EvaluationConfig:\n",
    "            config = self.config.evaluation\n",
    "            \n",
    "            create_directories([config.plots_dir, config.model_version_dir, config.data_version_dir])\n",
    "\n",
    "            evaluation_config = EvaluationConfig(\n",
    "                model_version_dir=Path(config.model_version_dir),\n",
    "                data_version_dir=Path(config.data_version_dir),\n",
    "                plots_dir=Path(config.plots_dir)\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Evaluation config: {config}\")\n",
    "            return evaluation_config\n",
    "\n",
    "        def get_cloud_storage_push_config(self) -> CloudStoragePushConfig:\n",
    "            config = self.config.cloud_storage_push\n",
    "            \n",
    "            cloud_storage_push_config = CloudStoragePushConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                bucket_name=config.bucket_name,\n",
    "                credentials_path=config.credentials_path,\n",
    "                data_version_dir=Path(config.data_version_dir),\n",
    "                evaluation_dir=Path(config.evaluation_dir),\n",
    "                workers=config.workers\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Cloud Storage Push config: {config}\")\n",
    "            return cloud_storage_push_config\n",
    "\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    \n",
    "    train_path_local, test_path_local, y_train_path_local, y_test_path_local = data_ingestion.data_ingestion_pipeline()\n",
    "    \n",
    "    logger.info(f\">>> Stage {STAGE_NAME} completed <<<\")\n",
    "    \n",
    "    \n",
    "    # Copy data to KFP artifacts\n",
    "    shutil.copy2(train_path_local, train_data.path)\n",
    "    shutil.copy2(test_path_local, test_data.path)\n",
    "    shutil.copy2(y_train_path_local, y_train_data.path)\n",
    "    shutil.copy2(y_test_path_local, y_test_data.path)\n",
    "    \n",
    "    logger.info(\">>> Stage Data Ingestion completed <<<\")\n",
    "    \n",
    "    outputs = namedtuple('Outputs', ['train_path', 'test_path', 'y_train_path', 'y_test_path'])\n",
    "    return outputs(train_data.path, test_data.path, y_train_data.path, y_test_data.path)\n",
    "\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"numpy\", \"joblib\"]\n",
    ")\n",
    "def model_preparation_component(\n",
    "    train_data: Input[Artifact],\n",
    "    test_data: Input[Artifact],\n",
    "    y_train_data: Input[Artifact],\n",
    "    y_test_data: Input[Artifact],\n",
    "    base_model: Output[Model],\n",
    "    scaler_artifact: Output[Artifact],\n",
    "    scaled_train: Output[Artifact],\n",
    "    scaled_test: Output[Artifact]\n",
    ") -> NamedTuple('Outputs', [('base_model_path', str), ('scaled_train_path', str), ('scaled_test_path', str), ('scaler_path', str)]):\n",
    "    \"\"\"Model preparation component using existing ModelPreparationStage\"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO, format='[%(asctime)s]: %(message)s:')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logs_dir = \"logs\"\n",
    "    log_filepath = os.path.join(logs_dir, \"running_logs.log\")\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        filename=log_filepath,\n",
    "        format=\"[%(asctime)s]: %(levelname)s: %(message)s\",\n",
    "        level=logging.INFO,\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filepath),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger(\"MLopsLogger\")\n",
    "\n",
    "    logger.info(\">>> Stage Prepare base model started <<<\")\n",
    "    STAGE_NAME = \"Prepare base model\"\n",
    "\n",
    "    logger.info(f\">>> Stage {STAGE_NAME} started <<<\")\n",
    "    class ConfigurationManager:\n",
    "        def __init__(\n",
    "            self,\n",
    "            config_filepath=Path(r\"C:\\Users\\Admin\\Desktop\\Data projects\\python\\Decision-making-system\\churn_mlops\\config\\config.yaml\")\n",
    "        ):\n",
    "            self.config = read_yaml(config_filepath)\n",
    "            create_directories([self.config.artifacts_root])\n",
    "\n",
    "            \n",
    "        def get_prepare_base_model_config(self) -> PrepareBaseModelConfig:\n",
    "            config = self.config.prepare_base_model\n",
    "            \n",
    "            create_directories([config.model_version_dir, config.data_version_dir])\n",
    "\n",
    "            prepare_base_model_config = PrepareBaseModelConfig(\n",
    "                model_version_dir=Path(config.model_version_dir),\n",
    "                data_version_dir=Path(config.data_version_dir),\n",
    "                n_estimators=config.n_estimators,\n",
    "                random_state=config.random_state,\n",
    "                criterion=config.criterion,\n",
    "                max_depth=config.max_depth,\n",
    "                max_features=config.max_features,\n",
    "                min_samples_leaf=config.min_samples_leaf\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Prepare base model config: {config}\")\n",
    "            return prepare_base_model_config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    prepare_base_model_config = ConfigurationManager().get_prepare_base_model_config()\n",
    "    prepare_base_model = PrepareBaseModel(config=prepare_base_model_config)\n",
    "\n",
    "    base_model_path_local, scaled_train_path_local, scaled_test_path_local, scaler_path_local = prepare_base_model.full_model(\n",
    "        train_path=train_data.path,\n",
    "        test_path=test_data.path,\n",
    "        y_train_path=y_train_data.path,\n",
    "        y_test_path=y_test_data.path\n",
    "    )\n",
    "\n",
    "    logger.info(f\">>> Stage {STAGE_NAME} completed <<<\")\n",
    "    # Call the existing pipeline function - matches main_pipeline.py exactly\n",
    "    \n",
    "    shutil.copy2(base_model_path_local, base_model.path)\n",
    "    shutil.copy2(scaler_path_local, scaler_artifact.path)\n",
    "    shutil.copy2(scaled_train_path_local, scaled_train.path)\n",
    "    shutil.copy2(scaled_test_path_local, scaled_test.path)\n",
    "    \n",
    "    logger.info(\">>> Stage Prepare base model completed <<<\")\n",
    "    \n",
    "    outputs = namedtuple('Outputs', ['base_model_path', 'scaled_train_path', 'scaled_test_path', 'scaler_path'])\n",
    "    return outputs(base_model.path, scaled_train.path, scaled_test.path, scaler_artifact.path)\n",
    "\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"numpy\", \"joblib\", \"matplotlib\", \"seaborn\"]\n",
    ")\n",
    "def train_evaluation_component(\n",
    "    base_model: Input[Model],\n",
    "    scaled_train: Input[Artifact],\n",
    "    scaled_test: Input[Artifact],\n",
    "    y_train_data: Input[Artifact],\n",
    "    y_test_data: Input[Artifact],\n",
    "    final_model: Output[Model],\n",
    "    metrics: Output[Metrics],\n",
    "    evaluation_plots: Output[Artifact]\n",
    ") -> NamedTuple('Outputs', [('final_model_path', str), ('accuracy', float), ('roc_auc', float)]):\n",
    "    \"\"\"Training and evaluation component using existing TrainEvaluationStage\"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO, format='[%(asctime)s]: %(message)s:')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logs_dir = \"logs\"\n",
    "    log_filepath = os.path.join(logs_dir, \"running_logs.log\")\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        filename=log_filepath,\n",
    "        format=\"[%(asctime)s]: %(levelname)s: %(message)s\",\n",
    "        level=logging.INFO,\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filepath),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger(\"MLopsLogger\")\n",
    "    \n",
    "    logger.info(\">>> Stage TRAIN_AND_EVALUATE_MODEL started <<<\")\n",
    "    STAGE_NAME = \"TRAIN_AND_EVALUATE_MODEL\"\n",
    "\n",
    "    logger.info(f\">>> Stage {STAGE_NAME} started <<<\")\n",
    "    class ConfigurationManager:\n",
    "        def __init__(\n",
    "            self,\n",
    "            config_filepath=Path(r\"C:\\Users\\Admin\\Desktop\\Data projects\\python\\Decision-making-system\\churn_mlops\\config\\config.yaml\")\n",
    "        ):\n",
    "            self.config = read_yaml(config_filepath)\n",
    "            create_directories([self.config.artifacts_root])\n",
    "\n",
    "        def get_training_config(self) -> TrainingConfig:\n",
    "            config = self.config.training\n",
    "            \n",
    "            create_directories([config.model_version_dir, config.data_version_dir])\n",
    "\n",
    "            training_config = TrainingConfig(\n",
    "                model_version_dir=Path(config.model_version_dir),\n",
    "                data_version_dir=Path(config.data_version_dir),\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Training config: {config}\")\n",
    "            return training_config\n",
    "\n",
    "        def get_evaluation_config(self) -> EvaluationConfig:\n",
    "            config = self.config.evaluation\n",
    "            \n",
    "            create_directories([config.plots_dir, config.model_version_dir, config.data_version_dir])\n",
    "\n",
    "            evaluation_config = EvaluationConfig(\n",
    "                model_version_dir=Path(config.model_version_dir),\n",
    "                data_version_dir=Path(config.data_version_dir),\n",
    "                plots_dir=Path(config.plots_dir)\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Evaluation config: {config}\")\n",
    "            return evaluation_config\n",
    "\n",
    "\n",
    "\n",
    "    training_config = ConfigurationManager().get_training_config()\n",
    "    evaluation_config = ConfigurationManager().get_evaluation_config()\n",
    "\n",
    "    model_processor = TrainAndEvaluateModel(\n",
    "        config_train=training_config,\n",
    "        config_eval=evaluation_config\n",
    "    )\n",
    "    \n",
    "    model, metrics_dict, final_model_path_local = model_processor.train_and_evaluate(\n",
    "        base_model_path=base_model.path,\n",
    "        scaled_train_path=scaled_train.path,\n",
    "        scaled_test_path=scaled_test.path,\n",
    "        y_train_path=y_train_data.path,\n",
    "        y_test_path=y_test_data.path\n",
    "    )\n",
    "    \n",
    "    logger.info(f\">>> Stage {STAGE_NAME} completed <<<\")\n",
    "\n",
    "    \n",
    "    # Copy final model to KFP artifact\n",
    "    shutil.copy2(final_model_path_local, final_model.path)\n",
    "    \n",
    "    # Log metrics to KFP\n",
    "    metrics.log_metric('accuracy', metrics_dict.get('accuracy', 0.0))\n",
    "    metrics.log_metric('roc_auc', metrics_dict.get('roc_auc', 0.0))\n",
    "    metrics.log_metric('precision', metrics_dict.get('precision', 0.0))\n",
    "    metrics.log_metric('recall', metrics_dict.get('recall', 0.0))\n",
    "    metrics.log_metric('f1_score', metrics_dict.get('f1_score', 0.0))\n",
    "    \n",
    "    # Create evaluation plots zip file from the plots directory\n",
    "    config_manager = ConfigurationManager()\n",
    "    eval_dir = config_manager.get_evaluation_config().plots_dir\n",
    "\n",
    "    if os.path.exists(eval_dir):\n",
    "        with zipfile.ZipFile(evaluation_plots.path, 'w') as zipf:\n",
    "            for root, _, files in os.walk(eval_dir):\n",
    "                for file in files:\n",
    "                    if file.endswith('.png') or file.endswith('.json'):\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        arcname = os.path.relpath(file_path, eval_dir)\n",
    "                        zipf.write(file_path, arcname)\n",
    "    else:\n",
    "        logger.warning(f\"Evaluation directory {eval_dir} not found. No plots will be zipped.\")\n",
    "    \n",
    "    logger.info(\">>> Stage TRAIN_AND_EVALUATE_MODEL completed <<<\")\n",
    "\n",
    "    outputs = namedtuple('Outputs', ['final_model_path', 'accuracy', 'roc_auc'])\n",
    "    return outputs(\n",
    "        final_model.path,\n",
    "        metrics_dict.get('accuracy', 0.0),\n",
    "        metrics_dict.get('roc_auc', 0.0)\n",
    "    )\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"boto3\", \"google-cloud-storage\"]\n",
    ")\n",
    "def cloud_storage_push_component() -> str:\n",
    "    \"\"\"Cloud storage push component using existing CloudStoragePushPipeline - matches main_pipeline.py exactly\"\"\"\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO, format='[%(asctime)s]: %(message)s:')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logs_dir = \"logs\"\n",
    "    log_filepath = os.path.join(logs_dir, \"running_logs.log\")\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        filename=log_filepath,\n",
    "        format=\"[%(asctime)s]: %(levelname)s: %(message)s\",\n",
    "        level=logging.INFO,\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filepath),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger(\"MLopsLogger\")\n",
    "    \n",
    "    logger.info(\">>> Stage Cloud Storage Push started <<<\")\n",
    "    logger.info(\"Starting cloud storage push component\")\n",
    "    class ConfigurationManager:\n",
    "        def __init__(\n",
    "            self,\n",
    "            config_filepath=Path(r\"C:\\Users\\Admin\\Desktop\\Data projects\\python\\Decision-making-system\\churn_mlops\\config\\config.yaml\")\n",
    "        ):\n",
    "            self.config = read_yaml(config_filepath)\n",
    "            create_directories([self.config.artifacts_root])\n",
    "\n",
    "        def get_cloud_storage_push_config(self) -> CloudStoragePushConfig:\n",
    "            config = self.config.cloud_storage_push\n",
    "            \n",
    "            cloud_storage_push_config = CloudStoragePushConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                bucket_name=config.bucket_name,\n",
    "                credentials_path=config.credentials_path,\n",
    "                data_version_dir=Path(config.data_version_dir),\n",
    "                evaluation_dir=Path(config.evaluation_dir),\n",
    "                workers=config.workers\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Cloud Storage Push config: {config}\")\n",
    "            return cloud_storage_push_config\n",
    "\n",
    "    config = ConfigurationManager()\n",
    "    cloud_storage_push_config = config.get_cloud_storage_push_config()\n",
    "    cloud_storage_push = CloudStoragePush(config=cloud_storage_push_config)\n",
    "    cloud_storage_push.push_to_cloud_storage()\n",
    "    \n",
    "    logger.info(\"Cloud storage push component completed\")\n",
    "\n",
    "    \n",
    "    \n",
    "    logger.info(\">>> Stage Cloud Storage Push completed <<<\")\n",
    "    return \"Cloud storage push completed successfully\"\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.11\"\n",
    ")\n",
    "def cleanup_component() -> str:\n",
    "    \"\"\"Cleanup component using existing cleanup logic from main_pipeline.py\"\"\"\n",
    "\n",
    "    import os\n",
    "    import sys\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO, format='[%(asctime)s]: %(message)s:')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logs_dir = \"logs\"\n",
    "    log_filepath = os.path.join(logs_dir, \"running_logs.log\")\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        filename=log_filepath,\n",
    "        format=\"[%(asctime)s]: %(levelname)s: %(message)s\",\n",
    "        level=logging.INFO,\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filepath),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    "    )\n",
    "    logger = logging.getLogger(\"MLopsLogger\")\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(\"CLEANUP: Removing temporary versioned files\")\n",
    "    logger.info(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        class ConfigurationManager:\n",
    "            def __init__(\n",
    "                self,\n",
    "                config_filepath=Path(r\"C:\\Users\\Admin\\Desktop\\Data projects\\python\\Decision-making-system\\churn_mlops\\config\\config.yaml\")\n",
    "            ):\n",
    "                self.config = read_yaml(config_filepath)\n",
    "                create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "            def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "                config = self.config.data_ingestion\n",
    "                \n",
    "                create_directories([config.root_dir, config.data_version_dir])\n",
    "\n",
    "                data_ingestion_config = DataIngestionConfig(\n",
    "                    root_dir=Path(config.root_dir),\n",
    "                    local_data_file=Path(config.local_data_file),\n",
    "                    test_size=config.test_size,\n",
    "                    random_state=config.random_state,\n",
    "                    data_version_dir=Path(config.data_version_dir)\n",
    "                )\n",
    "\n",
    "                logger.info(f\"Data Ingestion config: {config}\")\n",
    "                return data_ingestion_config\n",
    "                \n",
    "            def get_prepare_base_model_config(self) -> PrepareBaseModelConfig:\n",
    "                config = self.config.prepare_base_model\n",
    "                \n",
    "                create_directories([config.model_version_dir, config.data_version_dir])\n",
    "\n",
    "                prepare_base_model_config = PrepareBaseModelConfig(\n",
    "                    model_version_dir=Path(config.model_version_dir),\n",
    "                    data_version_dir=Path(config.data_version_dir),\n",
    "                    n_estimators=config.n_estimators,\n",
    "                    random_state=config.random_state,\n",
    "                    criterion=config.criterion,\n",
    "                    max_depth=config.max_depth,\n",
    "                    max_features=config.max_features,\n",
    "                    min_samples_leaf=config.min_samples_leaf\n",
    "                )\n",
    "\n",
    "                logger.info(f\"Prepare base model config: {config}\")\n",
    "                return prepare_base_model_config\n",
    "\n",
    "\n",
    "            def get_training_config(self) -> TrainingConfig:\n",
    "                config = self.config.training\n",
    "                \n",
    "                create_directories([config.model_version_dir, config.data_version_dir])\n",
    "\n",
    "                training_config = TrainingConfig(\n",
    "                    model_version_dir=Path(config.model_version_dir),\n",
    "                    data_version_dir=Path(config.data_version_dir),\n",
    "                )\n",
    "\n",
    "                logger.info(f\"Training config: {config}\")\n",
    "                return training_config\n",
    "\n",
    "            def get_evaluation_config(self) -> EvaluationConfig:\n",
    "                config = self.config.evaluation\n",
    "                \n",
    "                create_directories([config.plots_dir, config.model_version_dir, config.data_version_dir])\n",
    "\n",
    "                evaluation_config = EvaluationConfig(\n",
    "                    model_version_dir=Path(config.model_version_dir),\n",
    "                    data_version_dir=Path(config.data_version_dir),\n",
    "                    plots_dir=Path(config.plots_dir)\n",
    "                )\n",
    "\n",
    "                logger.info(f\"Evaluation config: {config}\")\n",
    "                return evaluation_config\n",
    "\n",
    "            def get_cloud_storage_push_config(self) -> CloudStoragePushConfig:\n",
    "                config = self.config.cloud_storage_push\n",
    "                \n",
    "                cloud_storage_push_config = CloudStoragePushConfig(\n",
    "                    root_dir=Path(config.root_dir),\n",
    "                    bucket_name=config.bucket_name,\n",
    "                    credentials_path=config.credentials_path,\n",
    "                    data_version_dir=Path(config.data_version_dir),\n",
    "                    evaluation_dir=Path(config.evaluation_dir),\n",
    "                    workers=config.workers\n",
    "                )\n",
    "\n",
    "                logger.info(f\"Cloud Storage Push config: {config}\")\n",
    "                return cloud_storage_push_config\n",
    "\n",
    "        config_manager = ConfigurationManager()\n",
    "        data_version_dir = config_manager.config.data_ingestion.data_version_dir\n",
    "        evaluation_dir = config_manager.config.evaluation.plots_dir\n",
    "        \n",
    "        # Pattern: *_version_YYYYMMDDTHHMMSS.csv\n",
    "        data_version_files = glob.glob(os.path.join(data_version_dir, \"*_version_????????T??????.csv\"))\n",
    "        logger.info(f\"Found {len(data_version_files)} timestamp-versioned data files to clean\")\n",
    "        \n",
    "        for file_path in data_version_files:\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                logger.info(f\"Deleted temporary file: {os.path.basename(file_path)}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to delete file {file_path}: {e}\")\n",
    "        \n",
    "        # Pattern: *_YYYYMMDDTHHMMSS.json and *_YYYYMMDDTHHMMSS.png\n",
    "        eval_json_files = glob.glob(os.path.join(evaluation_dir, \"*_????????T??????.json\"))\n",
    "        eval_png_files = glob.glob(os.path.join(evaluation_dir, \"*_????????T??????.png\"))\n",
    "        eval_files = eval_json_files + eval_png_files\n",
    "        logger.info(f\"Found {len(eval_files)} timestamp-versioned evaluation files to clean\")\n",
    "        \n",
    "        for file_path in eval_files:\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                logger.info(f\"Deleted temporary file: {os.path.basename(file_path)}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to delete file {file_path}: {e}\")\n",
    "        \n",
    "        remaining_data_files = len(glob.glob(os.path.join(data_version_dir, \"*.csv\")))\n",
    "        remaining_eval_files = len(glob.glob(os.path.join(evaluation_dir, \"*\")))\n",
    "        logger.info(f\"Kept {remaining_data_files} essential data files for DVC tracking\")\n",
    "        logger.info(f\"Kept {remaining_eval_files} essential evaluation files for DVC tracking\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error during cleanup: {e}\")\n",
    "    \n",
    "    logger.info(\"Cleanup completed\")\n",
    "    return \"Cleanup completed successfully\"\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='churn-prediction-pipeline',\n",
    "    description='Churn prediction pipeline with artifact tracking - matches main_pipeline.py logic'\n",
    ")\n",
    "def churn_prediction_pipeline():\n",
    "    \"\"\"Main pipeline definition with proper artifact tracking and same logic as main_pipeline.py\"\"\"\n",
    "    \n",
    "    # Step 1: Data preparation\n",
    "    data_prep_task = data_preparation_component()\n",
    "    \n",
    "    # Step 2: Model preparation\n",
    "    model_prep_task = model_preparation_component(\n",
    "        train_data=data_prep_task.outputs['train_data'],\n",
    "        test_data=data_prep_task.outputs['test_data'],\n",
    "        y_train_data=data_prep_task.outputs['y_train_data'],\n",
    "        y_test_data=data_prep_task.outputs['y_test_data']\n",
    "    )\n",
    "    model_prep_task.after(data_prep_task)\n",
    "    \n",
    "    # Step 3: Training and evaluation\n",
    "    train_eval_task = train_evaluation_component(\n",
    "        base_model=model_prep_task.outputs['base_model'],\n",
    "        scaled_train=model_prep_task.outputs['scaled_train'],\n",
    "        scaled_test=model_prep_task.outputs['scaled_test'],\n",
    "        y_train_data=data_prep_task.outputs['y_train_data'],\n",
    "        y_test_data=data_prep_task.outputs['y_test_data']\n",
    "    )\n",
    "    train_eval_task.after(model_prep_task)\n",
    "    \n",
    "    # Step 4: Cloud storage push\n",
    "    cloud_push_task = cloud_storage_push_component()\n",
    "    cloud_push_task.after(train_eval_task)\n",
    "    \n",
    "    # Step 5: Cleanup\n",
    "    cleanup_task = cleanup_component()\n",
    "    cleanup_task.after(cloud_push_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from kfp.compiler import Compiler\n",
    "from kfp import Client\n",
    "\n",
    "\n",
    "def reset_kubeflow_metadata():\n",
    "    \"\"\"Reset Kubeflow metadata to resolve context issues\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Resetting Kubeflow metadata store...\")\n",
    "        \n",
    "        # Connect to client and try to clear any stuck contexts\n",
    "        client = Client(host=\"http://localhost:8080\")\n",
    "        \n",
    "        # Get all experiments and clean up if needed\n",
    "        experiments = client.list_experiments()\n",
    "        logger.info(f\"Found {experiments.total_size if experiments else 0} experiments\")\n",
    "        \n",
    "        logger.info(\"Metadata reset completed\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not reset metadata (this is often normal): {e}\")\n",
    "        return True  # Continue anyway\n",
    "\n",
    "def compile_pipeline():\n",
    "    \"\"\"Compile the Kubeflow pipeline to YAML\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Compiling Kubeflow pipeline...\")\n",
    "        \n",
    "        compiler = Compiler()\n",
    "        compiler.compile(\n",
    "            pipeline_func=churn_prediction_pipeline,\n",
    "            package_path=\"churn_pipeline.yaml\"\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Pipeline compiled successfully: churn_pipeline.yaml\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error compiling pipeline: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def deploy_pipeline():\n",
    "    \"\"\"deployment with enhanced error handling and retry logic\"\"\"\n",
    "    max_retries = 3\n",
    "    retry_delay = 5\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            logger.info(f\"Deploying pipeline to Kubeflow (attempt {attempt + 1}/{max_retries})...\")\n",
    "            \n",
    "            # Connect to Kubeflow with timeout\n",
    "            client = Client(host=\"http://localhost:8080\")\n",
    "            \n",
    "            # Create unique run name with timestamp\n",
    "            run_name = f\"churn-prediction-{int(time.time())}-{attempt}\"\n",
    "            \n",
    "            # Create and run the pipeline\n",
    "            run = client.create_run_from_pipeline_package(\n",
    "                pipeline_file=\"churn_pipeline.yaml\",\n",
    "                arguments={},\n",
    "                run_name=run_name,\n",
    "                experiment_name=\"churn-prediction-experiments\"  # Create/use specific experiment\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"✅ Pipeline run created successfully: {run.run_id}\")\n",
    "            logger.info(f\"Monitor progress at: http://localhost:8080/#/runs/details/{run.run_id}\")\n",
    "            logger.info(\"Pipeline submitted to Kubeflow - check UI for status\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deploying pipeline (attempt {attempt + 1}): {e}\")\n",
    "            \n",
    "            if \"Cannot find context\" in str(e) or \"PipelineRun\" in str(e):\n",
    "                logger.info(\"Detected metadata context issue. Attempting reset...\")\n",
    "                reset_kubeflow_metadata()\n",
    "                \n",
    "            if attempt < max_retries - 1:\n",
    "                logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2  # Exponential backoff\n",
    "            else:\n",
    "                logger.error(\"All retry attempts failed\")\n",
    "                return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "def run_kubeflow_pipeline():\n",
    "    \"\"\"Main function to compile and run the Kubeflow pipeline with error handling\"\"\"\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"STARTING KUBEFLOW PIPELINE EXECUTION\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Reset metadata if needed\n",
    "    reset_kubeflow_metadata()\n",
    "    \n",
    "    # Step 2: Compile pipeline\n",
    "    if not compile_pipeline():\n",
    "        logger.error(\"Pipeline compilation failed. Exiting.\")\n",
    "        return False\n",
    "    \n",
    "    # Step 3: Deploy pipeline with retries\n",
    "    if not deploy_pipeline():\n",
    "        logger.error(\"Pipeline deployment failed after all retries. Exiting.\")\n",
    "        return False\n",
    "    \n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"KUBEFLOW PIPELINE DEPLOYMENT COMPLETED\")\n",
    "    logger.info(\"Check Kubeflow UI at http://localhost:8080 for execution status\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-03 16:22:56,453]: ============================================================:\n",
      "[2025-06-03 16:22:56,453]: STARTING KUBEFLOW PIPELINE EXECUTION:\n",
      "[2025-06-03 16:22:56,453]: ============================================================:\n",
      "[2025-06-03 16:22:56,453]: Resetting Kubeflow metadata store...:\n",
      "c:\\Users\\Admin\\Desktop\\Data projects\\python\\Decision-making-system\\myenv\\Lib\\site-packages\\kfp\\client\\client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n",
      "[2025-06-03 16:22:56,489]: Found 3 experiments:\n",
      "[2025-06-03 16:22:56,490]: Metadata reset completed:\n",
      "[2025-06-03 16:22:56,490]: Compiling Kubeflow pipeline...:\n",
      "[2025-06-03 16:22:56,512]: Pipeline compiled successfully: churn_pipeline.yaml:\n",
      "[2025-06-03 16:22:56,513]: Deploying pipeline to Kubeflow (attempt 1/3)...:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://localhost:8080/#/experiments/details/e35379fe-1b70-408a-9f85-b4777e5b7887\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://localhost:8080/#/runs/details/d0cfe869-a06d-4b9a-809a-36b8905d6a92\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-03 16:22:56,690]: ✅ Pipeline run created successfully: d0cfe869-a06d-4b9a-809a-36b8905d6a92:\n",
      "[2025-06-03 16:22:56,690]: Monitor progress at: http://localhost:8080/#/runs/details/d0cfe869-a06d-4b9a-809a-36b8905d6a92:\n",
      "[2025-06-03 16:22:56,692]: Pipeline submitted to Kubeflow - check UI for status:\n",
      "[2025-06-03 16:22:56,692]: ============================================================:\n",
      "[2025-06-03 16:22:56,693]: KUBEFLOW PIPELINE DEPLOYMENT COMPLETED:\n",
      "[2025-06-03 16:22:56,693]: Check Kubeflow UI at http://localhost:8080 for execution status:\n",
      "[2025-06-03 16:22:56,693]: ============================================================:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_kubeflow_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDefaultCredentialsError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m source_blob_name = \u001b[33m\"\u001b[39m\u001b[33mllmops-460406-f379299f4261.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m destination_file_name = \u001b[33m\"\u001b[39m\u001b[33mllmops-460406-f379299f4261.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mdownload_file_from_gcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_blob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination_file_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mdownload_file_from_gcs\u001b[39m\u001b[34m(bucket_name, source_blob_name, destination_file_name)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdownload_file_from_gcs\u001b[39m(bucket_name, source_blob_name, destination_file_name):\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Downloads a blob from the bucket.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     storage_client = \u001b[43mstorage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     bucket = storage_client.bucket(bucket_name)\n\u001b[32m      7\u001b[39m     blob = bucket.blob(source_blob_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Desktop\\Data projects\\python\\Decision-making-system\\myenv\\Lib\\site-packages\\google\\cloud\\storage\\client.py:247\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, project, credentials, _http, client_info, client_options, use_auth_w_custom_endpoint, extra_headers, api_key)\u001b[39m\n\u001b[32m    244\u001b[39m             no_project = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    245\u001b[39m             project = \u001b[33m\"\u001b[39m\u001b[33m<none>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_http\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_http\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# Validate that the universe domain of the credentials matches the\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# universe domain of the client.\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._credentials.universe_domain != \u001b[38;5;28mself\u001b[39m.universe_domain:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Desktop\\Data projects\\python\\Decision-making-system\\myenv\\Lib\\site-packages\\google\\cloud\\client\\__init__.py:338\u001b[39m, in \u001b[36mClientWithProject.__init__\u001b[39m\u001b[34m(self, project, credentials, client_options, _http)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, project=\u001b[38;5;28;01mNone\u001b[39;00m, credentials=\u001b[38;5;28;01mNone\u001b[39;00m, client_options=\u001b[38;5;28;01mNone\u001b[39;00m, _http=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     \u001b[43m_ClientProjectMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     Client.\u001b[34m__init__\u001b[39m(\n\u001b[32m    340\u001b[39m         \u001b[38;5;28mself\u001b[39m, credentials=credentials, client_options=client_options, _http=_http\n\u001b[32m    341\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Desktop\\Data projects\\python\\Decision-making-system\\myenv\\Lib\\site-packages\\google\\cloud\\client\\__init__.py:286\u001b[39m, in \u001b[36m_ClientProjectMixin.__init__\u001b[39m\u001b[34m(self, project, credentials)\u001b[39m\n\u001b[32m    283\u001b[39m     project = \u001b[38;5;28mgetattr\u001b[39m(credentials, \u001b[33m\"\u001b[39m\u001b[33mproject_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m project \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     project = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_determine_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m project \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    290\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mProject was not passed and could not be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    291\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdetermined from the environment.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    292\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Desktop\\Data projects\\python\\Decision-making-system\\myenv\\Lib\\site-packages\\google\\cloud\\client\\__init__.py:305\u001b[39m, in \u001b[36m_ClientProjectMixin._determine_default\u001b[39m\u001b[34m(project)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_determine_default\u001b[39m(project):\n\u001b[32m    304\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Helper:  use default project detection.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_determine_default_project\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Desktop\\Data projects\\python\\Decision-making-system\\myenv\\Lib\\site-packages\\google\\cloud\\_helpers\\__init__.py:152\u001b[39m, in \u001b[36m_determine_default_project\u001b[39m\u001b[34m(project)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Determine default project ID explicitly or implicitly as fall-back.\u001b[39;00m\n\u001b[32m    141\u001b[39m \n\u001b[32m    142\u001b[39m \u001b[33;03mSee :func:`google.auth.default` for details on how the default project\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    149\u001b[39m \u001b[33;03m:returns: Default project if it can be determined.\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m project \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     _, project = \u001b[43mgoogle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m project\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Desktop\\Data projects\\python\\Decision-making-system\\myenv\\Lib\\site-packages\\google\\auth\\_default.py:685\u001b[39m, in \u001b[36mdefault\u001b[39m\u001b[34m(scopes, request, quota_project_id, default_scopes)\u001b[39m\n\u001b[32m    677\u001b[39m             _LOGGER.warning(\n\u001b[32m    678\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mNo project ID could be determined. Consider running \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    679\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m`gcloud config set project` or setting the \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    680\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33menvironment variable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    681\u001b[39m                 environment_vars.PROJECT,\n\u001b[32m    682\u001b[39m             )\n\u001b[32m    683\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[31mDefaultCredentialsError\u001b[39m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def download_file_from_gcs(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(f\"Downloaded {source_blob_name} from bucket {bucket_name} to {destination_file_name}\")\n",
    "\n",
    "# Replace these with your bucket and file details\n",
    "bucket_name = \"churn_data_version\"\n",
    "source_blob_name = \"llmops-460406-f379299f4261.json\"\n",
    "destination_file_name = \"llmops-460406-f379299f4261.json\"\n",
    "\n",
    "download_file_from_gcs(bucket_name, source_blob_name, destination_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def download_gcs_file_with_secret():\n",
    "    key_path = \"/secrets/gcp/key.json\"\n",
    "    \n",
    "    # Check if the key file exists (to debug mount issues)\n",
    "    if not os.path.exists(key_path):\n",
    "        raise FileNotFoundError(f\"Service account key not found at {key_path}\")\n",
    "\n",
    "    # Initialize GCS client using the service account JSON\n",
    "    client = storage.Client.from_service_account_json(key_path)\n",
    "\n",
    "    # Define bucket and file names\n",
    "    bucket_name = \"churn_data_version\"\n",
    "    source_blob_name = \"input_raw.csv\"\n",
    "\n",
    "    # Create output filename with timestamp suffix\n",
    "    datetime_suffix = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "    destination_file_name = f\"input_raw_{datetime_suffix}.csv\"\n",
    "\n",
    "    # Download blob\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(f\"Downloaded '{source_blob_name}' from bucket '{bucket_name}' to local file '{destination_file_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import certifi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_input = pd.read_csv(\"https://raw.githubusercontent.com/Teungtran/churn_mlops/main/artifacts/data_ingestion/input_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "customer_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Purchase Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Product Category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Product Price",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Quantity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Total Purchase Amount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Payment Method",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Customer Age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Returns",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Customer Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Gender",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Churn",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Customer_Labels",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f021d1da-d044-49b7-908e-c76cb8643a4b",
       "rows": [
        [
         "0",
         "KH46251",
         "2020-09-08 09:38:32",
         "Electronics",
         "12",
         "3",
         "740",
         "Credit Card",
         "37",
         "0.0",
         "Christine Hernandez",
         "37",
         "Male",
         "0",
         "Lapsed Customers"
        ],
        [
         "1",
         "KH46251",
         "2022-03-05 12:56:35",
         "Home",
         "468",
         "4",
         "2739",
         "PayPal",
         "37",
         "0.0",
         "Christine Hernandez",
         "37",
         "Male",
         "0",
         "Lapsed Customers"
        ],
        [
         "2",
         "KH46251",
         "2022-05-23 18:18:01",
         "Home",
         "288",
         "2",
         "3196",
         "PayPal",
         "37",
         "0.0",
         "Christine Hernandez",
         "37",
         "Male",
         "0",
         "Lapsed Customers"
        ],
        [
         "3",
         "KH46251",
         "2020-11-12 13:13:29",
         "Clothing",
         "196",
         "1",
         "3509",
         "PayPal",
         "37",
         "0.0",
         "Christine Hernandez",
         "37",
         "Male",
         "0",
         "Lapsed Customers"
        ],
        [
         "4",
         "KH13593",
         "2020-11-27 17:55:11",
         "Home",
         "449",
         "1",
         "3452",
         "Credit Card",
         "49",
         "0.0",
         "James Grant",
         "49",
         "Female",
         "1",
         "Regular Customers"
        ],
        [
         "5",
         "KH13593",
         "2023-03-07 14:17:42",
         "Home",
         "250",
         "4",
         "575",
         "PayPal",
         "49",
         "1.0",
         "James Grant",
         "49",
         "Female",
         "1",
         "Regular Customers"
        ],
        [
         "6",
         "KH13593",
         "2023-04-15 03:02:33",
         "Electronics",
         "73",
         "1",
         "1896",
         "Credit Card",
         "49",
         "0.0",
         "James Grant",
         "49",
         "Female",
         "1",
         "Regular Customers"
        ],
        [
         "7",
         "KH13593",
         "2021-03-27 21:23:28",
         "Books",
         "337",
         "2",
         "2937",
         "Cash",
         "49",
         "0.0",
         "James Grant",
         "49",
         "Female",
         "1",
         "Regular Customers"
        ],
        [
         "8",
         "KH13593",
         "2020-05-05 20:14:00",
         "Clothing",
         "182",
         "2",
         "3363",
         "PayPal",
         "49",
         "1.0",
         "James Grant",
         "49",
         "Female",
         "1",
         "Regular Customers"
        ],
        [
         "9",
         "KH28805",
         "2023-09-13 04:24:00",
         "Electronics",
         "394",
         "2",
         "1993",
         "Credit Card",
         "19",
         "0.0",
         "Jose Collier",
         "19",
         "Male",
         "0",
         "Regular Customers"
        ],
        [
         "10",
         "KH28805",
         "2021-03-31 09:50:57",
         "Clothing",
         "366",
         "1",
         "246",
         "PayPal",
         "19",
         "0.0",
         "Jose Collier",
         "19",
         "Male",
         "0",
         "Regular Customers"
        ],
        [
         "11",
         "KH28805",
         "2021-01-18 22:42:24",
         "Books",
         "348",
         "1",
         "2682",
         "Credit Card",
         "19",
         "1.0",
         "Jose Collier",
         "19",
         "Male",
         "0",
         "Regular Customers"
        ],
        [
         "12",
         "KH28805",
         "2020-01-07 12:57:35",
         "Books",
         "103",
         "4",
         "731",
         "Cash",
         "19",
         "0.0",
         "Jose Collier",
         "19",
         "Male",
         "0",
         "Regular Customers"
        ],
        [
         "13",
         "KH28805",
         "2021-02-12 20:33:20",
         "Books",
         "240",
         "1",
         "2563",
         "PayPal",
         "19",
         "0.0",
         "Jose Collier",
         "19",
         "Male",
         "0",
         "Regular Customers"
        ],
        [
         "14",
         "KH28805",
         "2020-07-02 02:54:37",
         "Clothing",
         "368",
         "1",
         "1342",
         "Credit Card",
         "19",
         null,
         "Jose Collier",
         "19",
         "Male",
         "0",
         "Regular Customers"
        ],
        [
         "15",
         "KH28961",
         "2021-04-25 23:55:21",
         "Books",
         "30",
         "1",
         "4135",
         "PayPal",
         "55",
         null,
         "James Stein",
         "55",
         "Male",
         "0",
         "Regular Customers"
        ],
        [
         "16",
         "KH28961",
         "2020-01-13 09:57:17",
         "Books",
         "153",
         "5",
         "698",
         "Credit Card",
         "55",
         "0.0",
         "James Stein",
         "55",
         "Male",
         "0",
         "Regular Customers"
        ],
        [
         "17",
         "KH28961",
         "2023-06-18 21:34:27",
         "Clothing",
         "259",
         "1",
         "2975",
         "Credit Card",
         "55",
         "0.0",
         "James Stein",
         "55",
         "Male",
         "0",
         "Regular Customers"
        ],
        [
         "18",
         "KH28961",
         "2021-09-10 00:39:41",
         "Books",
         "489",
         "3",
         "2213",
         "Credit Card",
         "55",
         "1.0",
         "James Stein",
         "55",
         "Male",
         "0",
         "Regular Customers"
        ],
        [
         "19",
         "KH28961",
         "2023-06-01 19:07:10",
         "Books",
         "232",
         "3",
         "4452",
         "PayPal",
         "55",
         "1.0",
         "James Stein",
         "55",
         "Male",
         "0",
         "Regular Customers"
        ],
        [
         "20",
         "KH12163",
         "2021-12-18 17:49:18",
         "Clothing",
         "255",
         "2",
         "1642",
         "Credit Card",
         "67",
         null,
         "Sonia Moreno",
         "67",
         "Male",
         "0",
         "VIP Customers"
        ],
        [
         "21",
         "KH12163",
         "2020-06-20 05:56:17",
         "Books",
         "227",
         "3",
         "887",
         "PayPal",
         "67",
         null,
         "Sonia Moreno",
         "67",
         "Male",
         "0",
         "VIP Customers"
        ],
        [
         "22",
         "KH12163",
         "2023-07-08 13:36:47",
         "Clothing",
         "288",
         "2",
         "1405",
         "PayPal",
         "67",
         "0.0",
         "Sonia Moreno",
         "67",
         "Male",
         "0",
         "VIP Customers"
        ],
        [
         "23",
         "KH12163",
         "2023-07-01 12:36:00",
         "Books",
         "60",
         "3",
         "4130",
         "Cash",
         "67",
         null,
         "Sonia Moreno",
         "67",
         "Male",
         "0",
         "VIP Customers"
        ],
        [
         "24",
         "KH12163",
         "2022-11-16 08:56:03",
         "Home",
         "285",
         "4",
         "384",
         "Crypto",
         "67",
         null,
         "Sonia Moreno",
         "67",
         "Male",
         "0",
         "VIP Customers"
        ],
        [
         "25",
         "KH12163",
         "2020-04-07 04:24:44",
         "Books",
         "100",
         "5",
         "1273",
         "Cash",
         "67",
         "0.0",
         "Sonia Moreno",
         "67",
         "Male",
         "0",
         "VIP Customers"
        ],
        [
         "26",
         "KH12163",
         "2020-11-28 16:48:01",
         "Electronics",
         "28",
         "4",
         "495",
         "Credit Card",
         "67",
         "0.0",
         "Sonia Moreno",
         "67",
         "Male",
         "0",
         "VIP Customers"
        ],
        [
         "27",
         "KH12163",
         "2020-08-21 09:23:52",
         "Electronics",
         "382",
         "2",
         "1532",
         "PayPal",
         "67",
         "1.0",
         "Sonia Moreno",
         "67",
         "Male",
         "0",
         "VIP Customers"
        ],
        [
         "28",
         "KH12163",
         "2023-04-24 23:55:44",
         "Home",
         "400",
         "5",
         "1673",
         "Credit Card",
         "67",
         "1.0",
         "Sonia Moreno",
         "67",
         "Male",
         "0",
         "VIP Customers"
        ],
        [
         "29",
         "KH28248",
         "2023-04-09 01:33:30",
         "Clothing",
         "410",
         "1",
         "445",
         "Cash",
         "48",
         null,
         "Kelly Ramirez",
         "48",
         "Male",
         "1",
         "VIP Customers"
        ],
        [
         "30",
         "KH28248",
         "2020-04-26 21:19:56",
         "Books",
         "70",
         "5",
         "1037",
         "PayPal",
         "48",
         "0.0",
         "Kelly Ramirez",
         "48",
         "Male",
         "1",
         "VIP Customers"
        ],
        [
         "31",
         "KH28248",
         "2023-09-12 20:36:28",
         "Home",
         "243",
         "3",
         "2516",
         "PayPal",
         "48",
         "0.0",
         "Kelly Ramirez",
         "48",
         "Male",
         "1",
         "VIP Customers"
        ],
        [
         "32",
         "KH28248",
         "2021-06-25 14:52:44",
         "Home",
         "131",
         "4",
         "2019",
         "PayPal",
         "48",
         "0.0",
         "Kelly Ramirez",
         "48",
         "Male",
         "1",
         "VIP Customers"
        ],
        [
         "33",
         "KH28248",
         "2021-11-24 07:13:08",
         "Home",
         "327",
         "1",
         "1496",
         "Cash",
         "48",
         null,
         "Kelly Ramirez",
         "48",
         "Male",
         "1",
         "VIP Customers"
        ],
        [
         "34",
         "KH28248",
         "2022-09-22 17:12:16",
         "Books",
         "405",
         "3",
         "1687",
         "PayPal",
         "48",
         "1.0",
         "Kelly Ramirez",
         "48",
         "Male",
         "1",
         "VIP Customers"
        ],
        [
         "35",
         "KH28248",
         "2021-08-03 09:41:40",
         "Electronics",
         "219",
         "2",
         "468",
         "PayPal",
         "48",
         "0.0",
         "Kelly Ramirez",
         "48",
         "Male",
         "1",
         "VIP Customers"
        ],
        [
         "36",
         "KH6761",
         "2021-11-29 12:56:23",
         "Books",
         "218",
         "5",
         "3034",
         "Credit Card",
         "46",
         "0.0",
         "Johnny Peterson",
         "46",
         "Female",
         "0",
         "Lapsed Customers"
        ],
        [
         "37",
         "KH6761",
         "2020-11-23 05:27:32",
         "Clothing",
         "304",
         "4",
         "3110",
         "Cash",
         "46",
         null,
         "Johnny Peterson",
         "46",
         "Female",
         "0",
         "Lapsed Customers"
        ],
        [
         "38",
         "KH6761",
         "2020-01-08 05:34:28",
         "Clothing",
         "20",
         "4",
         "3075",
         "PayPal",
         "46",
         "0.0",
         "Johnny Peterson",
         "46",
         "Female",
         "0",
         "Lapsed Customers"
        ],
        [
         "39",
         "KH6761",
         "2021-05-10 17:46:35",
         "Home",
         "400",
         "2",
         "1036",
         "Cash",
         "46",
         null,
         "Johnny Peterson",
         "46",
         "Female",
         "0",
         "Lapsed Customers"
        ],
        [
         "40",
         "KH17018",
         "2023-06-27 03:47:09",
         "Home",
         "227",
         "2",
         "1953",
         "Cash",
         "21",
         "1.0",
         "Jerry Mendoza",
         "21",
         "Male",
         "0",
         "VIP Customers"
        ],
        [
         "41",
         "KH17018",
         "2022-01-17 11:15:32",
         "Clothing",
         "285",
         "3",
         "4630",
         "Credit Card",
         "21",
         "1.0",
         "Jerry Mendoza",
         "21",
         "Male",
         "0",
         "VIP Customers"
        ],
        [
         "42",
         "KH17018",
         "2022-03-03 04:41:32",
         "Books",
         "31",
         "1",
         "268",
         "Credit Card",
         "21",
         "0.0",
         "Jerry Mendoza",
         "21",
         "Male",
         "0",
         "VIP Customers"
        ],
        [
         "43",
         "KH17018",
         "2020-05-10 00:33:28",
         "Books",
         "434",
         "4",
         "456",
         "PayPal",
         "21",
         "0.0",
         "Jerry Mendoza",
         "21",
         "Male",
         "0",
         "VIP Customers"
        ],
        [
         "44",
         "KH17018",
         "2022-12-09 08:32:41",
         "Home",
         "156",
         "3",
         "3937",
         "Credit Card",
         "21",
         "0.0",
         "Jerry Mendoza",
         "21",
         "Male",
         "0",
         "VIP Customers"
        ],
        [
         "45",
         "KH17018",
         "2021-04-30 13:16:25",
         "Clothing",
         "119",
         "3",
         "3967",
         "PayPal",
         "21",
         null,
         "Jerry Mendoza",
         "21",
         "Male",
         "0",
         "VIP Customers"
        ],
        [
         "46",
         "KH17018",
         "2020-06-25 00:15:16",
         "Electronics",
         "53",
         "4",
         "3729",
         "PayPal",
         "21",
         "1.0",
         "Jerry Mendoza",
         "21",
         "Male",
         "0",
         "VIP Customers"
        ],
        [
         "47",
         "KH11462",
         "2020-01-12 09:59:22",
         "Clothing",
         "485",
         "3",
         "509",
         "PayPal",
         "32",
         "1.0",
         "Cory Wagner",
         "32",
         "Female",
         "0",
         "VIP Customers"
        ],
        [
         "48",
         "KH11462",
         "2023-01-12 07:48:01",
         "Home",
         "191",
         "4",
         "1785",
         "PayPal",
         "32",
         null,
         "Cory Wagner",
         "32",
         "Female",
         "0",
         "VIP Customers"
        ],
        [
         "49",
         "KH11462",
         "2022-01-04 00:05:56",
         "Books",
         "31",
         "5",
         "1464",
         "Cash",
         "32",
         "0.0",
         "Cory Wagner",
         "32",
         "Female",
         "0",
         "VIP Customers"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 204163
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>Purchase Date</th>\n",
       "      <th>Product Category</th>\n",
       "      <th>Product Price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Total Purchase Amount</th>\n",
       "      <th>Payment Method</th>\n",
       "      <th>Customer Age</th>\n",
       "      <th>Returns</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Churn</th>\n",
       "      <th>Customer_Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KH46251</td>\n",
       "      <td>2020-09-08 09:38:32</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>740</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Christine Hernandez</td>\n",
       "      <td>37</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>Lapsed Customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KH46251</td>\n",
       "      <td>2022-03-05 12:56:35</td>\n",
       "      <td>Home</td>\n",
       "      <td>468</td>\n",
       "      <td>4</td>\n",
       "      <td>2739</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Christine Hernandez</td>\n",
       "      <td>37</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>Lapsed Customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KH46251</td>\n",
       "      <td>2022-05-23 18:18:01</td>\n",
       "      <td>Home</td>\n",
       "      <td>288</td>\n",
       "      <td>2</td>\n",
       "      <td>3196</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Christine Hernandez</td>\n",
       "      <td>37</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>Lapsed Customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KH46251</td>\n",
       "      <td>2020-11-12 13:13:29</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "      <td>3509</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Christine Hernandez</td>\n",
       "      <td>37</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>Lapsed Customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KH13593</td>\n",
       "      <td>2020-11-27 17:55:11</td>\n",
       "      <td>Home</td>\n",
       "      <td>449</td>\n",
       "      <td>1</td>\n",
       "      <td>3452</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>James Grant</td>\n",
       "      <td>49</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>Regular Customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204158</th>\n",
       "      <td>KH17165</td>\n",
       "      <td>2020-02-25 13:38:16</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>230</td>\n",
       "      <td>4</td>\n",
       "      <td>3664</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dawn Perez</td>\n",
       "      <td>18</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>Lapsed Customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204159</th>\n",
       "      <td>KH45397</td>\n",
       "      <td>2022-02-18 04:18:18</td>\n",
       "      <td>Books</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "      <td>3397</td>\n",
       "      <td>Cash</td>\n",
       "      <td>54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scott Lindsey</td>\n",
       "      <td>54</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>Lapsed Customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204160</th>\n",
       "      <td>KH45410</td>\n",
       "      <td>2021-05-30 15:37:15</td>\n",
       "      <td>Home</td>\n",
       "      <td>311</td>\n",
       "      <td>2</td>\n",
       "      <td>3302</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Johnny Riley</td>\n",
       "      <td>50</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>Lapsed Customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204161</th>\n",
       "      <td>KH48835</td>\n",
       "      <td>2021-11-23 01:30:42</td>\n",
       "      <td>Home</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>3615</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Rush</td>\n",
       "      <td>42</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>Lapsed Customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204162</th>\n",
       "      <td>KH16971</td>\n",
       "      <td>2021-03-13 16:28:35</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>425</td>\n",
       "      <td>4</td>\n",
       "      <td>2370</td>\n",
       "      <td>Cash</td>\n",
       "      <td>36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Justin Lawson</td>\n",
       "      <td>36</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>Lapsed Customers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204163 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       customer_id        Purchase Date Product Category  Product Price  \\\n",
       "0          KH46251  2020-09-08 09:38:32      Electronics             12   \n",
       "1          KH46251  2022-03-05 12:56:35             Home            468   \n",
       "2          KH46251  2022-05-23 18:18:01             Home            288   \n",
       "3          KH46251  2020-11-12 13:13:29         Clothing            196   \n",
       "4          KH13593  2020-11-27 17:55:11             Home            449   \n",
       "...            ...                  ...              ...            ...   \n",
       "204158     KH17165  2020-02-25 13:38:16         Clothing            230   \n",
       "204159     KH45397  2022-02-18 04:18:18            Books             95   \n",
       "204160     KH45410  2021-05-30 15:37:15             Home            311   \n",
       "204161     KH48835  2021-11-23 01:30:42             Home             27   \n",
       "204162     KH16971  2021-03-13 16:28:35      Electronics            425   \n",
       "\n",
       "        Quantity  Total Purchase Amount Payment Method  Customer Age  Returns  \\\n",
       "0              3                    740    Credit Card            37      0.0   \n",
       "1              4                   2739         PayPal            37      0.0   \n",
       "2              2                   3196         PayPal            37      0.0   \n",
       "3              1                   3509         PayPal            37      0.0   \n",
       "4              1                   3452    Credit Card            49      0.0   \n",
       "...          ...                    ...            ...           ...      ...   \n",
       "204158         4                   3664    Credit Card            18      0.0   \n",
       "204159         2                   3397           Cash            54      NaN   \n",
       "204160         2                   3302    Credit Card            50      1.0   \n",
       "204161         1                   3615    Credit Card            42      1.0   \n",
       "204162         4                   2370           Cash            36      1.0   \n",
       "\n",
       "              Customer Name  Age  Gender  Churn    Customer_Labels  \n",
       "0       Christine Hernandez   37    Male      0   Lapsed Customers  \n",
       "1       Christine Hernandez   37    Male      0   Lapsed Customers  \n",
       "2       Christine Hernandez   37    Male      0   Lapsed Customers  \n",
       "3       Christine Hernandez   37    Male      0   Lapsed Customers  \n",
       "4               James Grant   49  Female      1  Regular Customers  \n",
       "...                     ...  ...     ...    ...                ...  \n",
       "204158           Dawn Perez   18    Male      0   Lapsed Customers  \n",
       "204159        Scott Lindsey   54    Male      0   Lapsed Customers  \n",
       "204160         Johnny Riley   50    Male      0   Lapsed Customers  \n",
       "204161          Jeremy Rush   42  Female      1   Lapsed Customers  \n",
       "204162        Justin Lawson   36  Female      1   Lapsed Customers  \n",
       "\n",
       "[204163 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
